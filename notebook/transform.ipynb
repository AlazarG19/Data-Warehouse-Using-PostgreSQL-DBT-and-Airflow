{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "# Increase the maximum field size limit\n",
    "csv.field_size_limit(10**8)\n",
    "import psycopg2\n",
    "from psycopg2 import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_into_parts(input_list, part_size):\n",
    "    return [input_list[i:i + part_size] for i in range(0, len(input_list), part_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "file_path = '../data/20181024_d1_0830_0900.csv'\n",
    "\n",
    "# Open the CSV file\n",
    "with open(file_path, newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    \n",
    "    i = 0\n",
    "    # Read each row one by one\n",
    "    for line_number, row in enumerate(csv_reader):\n",
    "        # Process individual lines as needed\n",
    "        if(i == 10):\n",
    "            break\n",
    "        print(i)\n",
    "        if (i == 0):\n",
    "            # create the column names for the fact and dimensional table\n",
    "            columns = row[0]\n",
    "            dimension_columns = [columns.split(\";\")[0]] + columns.split(\";\")[4:]\n",
    "            \n",
    "            # create fact and dimensional table\n",
    "            fact_df = pd.DataFrame(columns=columns.split(\";\")[:4])\n",
    "            dimension_df = pd.DataFrame(columns=dimension_columns)          \n",
    "        else:\n",
    "            data = [elt  for elt in row[0].split(\";\") if elt != \" \"]\n",
    "            track_id = data[0]\n",
    "            \n",
    "            fact_df.loc[len(fact_df)] = data[:4]\n",
    "            \n",
    "            # Split the list into parts containing 6 elements each\n",
    "            split_parts = split_list_into_parts(data[4:], 6)\n",
    "\n",
    "            for dimensions in split_parts:\n",
    "                dimensions.insert(0, track_id)\n",
    "                dimension_df.loc[len(dimension_df)] = dimensions\n",
    "        i+=1     \n",
    "        fact_df.to_csv(\"../data/fact_df.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fact_df.shape)\n",
    "print(dimension_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df.track_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the paths to your CSV files\n",
    "fact_csv_path = '../data/fact_df.csv'  # Change this to your actual CSV path\n",
    "dimensional_csv_path = '../data/dimension_df.csv'  # Change this to your actual CSV path\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "fact_df = pd.read_csv(fact_csv_path)\n",
    "dimensional_df = pd.read_csv(dimensional_csv_path)\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    \n",
    "            host=\"172.31.98.138\",\n",
    "            database=\"airflow_traffic_db\",\n",
    "            port=\"5432\",\n",
    "            user=\"postgres\",\n",
    "            password=\"password\"\n",
    "        )\n",
    "cursor = conn.cursor()\n",
    "print(fact_df.columns)\n",
    "print(dimensional_df.columns)\n",
    "\n",
    "# Insert data into fact_df\n",
    "for index, row in fact_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO fact_df (track_id, type, traveled_d, avg_speed) \n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['track_id'], row['type'], row['traveled_d'], row['avg_speed']))\n",
    "\n",
    "# Insert data into dimensional_df\n",
    "for index, row in dimensional_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO dimensional_df (track_id, lat, lon, speed, ion_acc, lat_acc, time) \n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['track_id'], row['lat'], row['lon'], row['speed'], row['ion_acc'], row['lat_acc'], row['time']))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
